#!/bin/bash

# Job script for Great Lakes cluster at University of Michigan.
#
# To compile and run SWMF on Great Lakes use the Intel Compilers and IMPI.
# Run this to make sure the modules run every login and job start:
#
#	echo "module load intel && module load impi" >> ~/.profile
#
# Customize this file and run inside the run directory
#
#	cd run
#	sbatch job.gl-login
#
# It is wize to copy the run directory to the scratch root folder
# associated with your Slurm account. Files tend to get large and will
# fill your home folder fast.
#
# To avoid having too many output files run
#
# 	./PostProc.pl -g -r=60 -v &> PostProc.log &
#
# You may run this outside of a Slurm batch however make sure you
# disown it before logging out to ensure it runs in the background
# while you are away
# 
# 	disown -a && exit
# 
# Useful Slurm commands:
# 
# To view information about your job and possible start time run:
# 
# 	squeue --account=test
# or
# 	squeue --users=user
# 
# where `test` or `user` would be replaced by your Slurm account name
# or username respectively. Add `--start` to see prospective start time.
# 
# To kill your job
# 
# 	scancel --user=user jobid
# 
# Where user is your username and jobid is the job ID that you want to
# cancel.
# 

## Replace `test` with the Slurm account this job will run under.
#SBATCH --account=test

## Job name
#SBATCH --job-name=SWMF

## Configuration
# Number of nodes
# min-max (helps with start times)
#SBATCH --nodes=16-32

# Number of processes per node
#SBATCH --ntasks-per-node=36

# Number of cores or threads per node
# (might be needed for OpenMP)
#SBATCH --cpus-per-task=1

# Memory per cpu
#SBATCH --mem-per-cpu=1g

# Wall time HH:MM:SS (max 2 weeks)
#SBATCH --time=02:00:00

# Either standard, largemem, or gpu
#SBATCH --partition=standard

# The emails you will receive about your job
#SBATCH --mail-type=BEGIN,END,FAIL

# Output file
#SBATCH --output=runlog_%x_id%j
#SBATCH --error=runerr_%x_id%j

# Needed to forward user environment
#SBATCH --get-user-env
source ~/.bashrc
source ~/.bash_profile

# Begin at specific time
# #SBATCH --begin=2020-12-25T12:30:00
# Commented to run ASAP

# These settings may or may not be useful
# #SBATCH --export=MPI_MSGS_PER_HOST=100000
# #SBATCH --export=MPI_MSGS_PER_PROC=100000
# #SBATCH --export=MPI_MSGS_MAX=100000

# Seems to be needed for HDF5 plots
# #SBATCH --export=MPI_TYPE_DEPTH=20

# Running with OpenMP may require the following settings
# (For the SWMF the LAYOUT.in may be able to achieve something similar)
# #SBATCH --export=MPI_DSM_DISTRIBUTE=0
# #SBATCH --export=KMP_AFFINITY=disabled
# #SBATCH --export=OMP_NUM_THREADS=36

## Run
printf "START TIME: `date +%Y%m%d`\n"
# Run SWMF (the number of processors is already specified above)
env srun --export=ALL ./SWMF.exe

# To use automated resubmission adapt the script below!

# Use the #CPUTIMEMAX and #CHECKSTOP commands in PARAM.in
# so the code stops before the wall clock time is exceeded.

# Link latest restart files
# ./Restart.pl

# Provide a PARAM.in.restart file if you wish and uncomment these lines:
# cp PARAM.in PARAM.in.init
# cp PARAM.in.restart PARAM.in

# Resubmit job
# Use another job script if you don't want it to resubmit forever.
# sbatch --dependency=afterany:$SLURM_JOB_ID  job.gl-login
