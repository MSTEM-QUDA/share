#!/bin/bash

# Use default modules. To use IDL,
# module load idl
#
# Compilation:
# to avoid using too many threads on the login node, compile code with 
# make -j 4
#
# Move run directory to the $SCRATCH disk:
# make rundir
# mv run $SCRATCH/run1
# ln -s $SCRATCH/run1 .
#
# submit the jobscript:
# sbatch job.frontera
#
# monitor jobs:
# squeue -u USERNAME
#
# Continuous post-processing has to be done on the compute nodes with
# sbatch job.postproc.frontera

#SBATCH -J sub1               # Job name
#SBATCH -o SWMF.o%j           # Name of stdout output file
#SBATCH -e SWMF.e%j           # Name of stderr error file
#SBATCH -p normal             # Queue (partition) name: normal or development
#SBATCH -N 64                 # Total # of nodes 
#SBATCH --tasks-per-node 56   # Number of MPI tasks per node. 
#SBATCH -t 24:00:00           # Run time (hh:mm:ss)
#SBATCH --mail-type=all       # Send email at begin and end of job
###SBATCH --mail-user=your_email@umich.edu
###SBATCH -A myproject       # Project/Allocation name (req'd if you have more than 1)

# Any other commands must follow all #SBATCH directives...

# By 05/2020, the default MPI_Reduce algorithm in IMPI may
# produce wrong results. For example, reducing a large
# array of ~30,000 integers on 2000 MPIs does not work correctly. This bug
# can be avoided by using another MPI_Reduce algorithm. The environment
# variable I_MPI_ADJUST_REDUCE is used to select the algorithm, and 
# either I_MPI_ADJUST_REDUCE=1 or I_MPI_ADJUST_REDUCE=3 works. The
# following line can also be copied to .bashrc to select algorithm for
# an interactive session:
export I_MPI_ADJUST_REDUCE=1 

# IMPI always produces warning messages for the jobs with >128 MPIs.
# The following line suppresses such warnings. 
export UCX_LOG_LEVEL=ERROR 

# Launch MPI code... 
# Use ibrun instead of mpirun or mpiexec
ibrun ./SWMF.exe  > runlog_`date +%y%m%d%H%M`

